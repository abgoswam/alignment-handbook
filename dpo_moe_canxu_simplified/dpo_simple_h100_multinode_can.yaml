description: Distributed_Simple DPO

target:  
  service: sing  
  name: GenAI-Shared-CanadaCtr #aims-sing-east-us2  
  workspace_name: aims-sing-res-eu-WS

environment:
  image: amlt-sing/acpt-2.2.1-py3.10-cuda12.1 #amlt-sing/acpt-pytorch-2.0.1-py3.10-cuda11.8
  setup:
    - nvcc --version
    - pip list
    - echo "using a pre-built curated image"
    - ls -a
    - pip install -r requirements.txt
    - pip install trl==0.8.5
    - pip install flash-attn==2.3.6 --no-build-isolation

code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: $CONFIG_DIR


storage:
  posttraining:
    storage_account_name: customdataws9388857230
    container_name: post-training
    mount_dir: /mnt/posttraining
  aimsllmeus2users:
    storage_account_name: aimsllmeus2
    container_name: users
    mount_dir: /mnt/aimsllmeus2users

jobs:
- name: dpo_issue_1078_mixtral8x7_can
  sku: 4xG8-H100-IB
  mpi: True
  process_count_per_node: 1
  # priority: Medium
  priority: High
  # sla_tier: Standard
  # --model_name_or_path /mnt/posttraining/ckpts/mistralai_Mixtral-8x7B-v0.1
  # --model_name_or_path /mnt/posttraining/ckpts/phi_moe_hf/123000_hf_bfloat16_04172024
  sla_tier: Premium
  command:
  - pip install azureml-sdk
  - printenv | grep -i rank  
  - nvcc --version
  - pip list
  

  - ACCELERATE_LOG_LEVEL=info accelerate launch 
    --num_processes 32 
    --num_machines 4 
    --main_process_ip $${MASTER_ADDR} 
    --main_process_port $${MASTER_PORT}  
    --machine_rank $${NODE_RANK}  
    --config_file src/configs/deepspeed_zero3_offload_nodes.yaml 
    src/dpo_train_multinode.py 
    --model_name_or_path /mnt/posttraining/ckpts/mistralai_Mixtral-8x7B-v0.1
    --json_path data/ultra_60k.json 
    --data_split train 
    --output_dir /mnt/posttraining/agoswami/dpo_model/phimoe  
    --num_train_epochs 3 
    --beta 0.07 
    --model_max_length 2048  
    --per_device_train_batch_size 4  
    --per_device_eval_batch_size 1  
    --gradient_accumulation_steps 4  
    --save_global_steps False 
    --eval_steps 50 
    --save_strategy "steps"  
    --save_steps 38  
    --save_total_limit 25  
    --learning_rate 5e-7  
    --warmup_ratio 0.1 
    --logging_steps 1  
    --lr_scheduler_type "linear"  
    --do_eval False 
    --evaluation_strategy "no"  
    --conv_template "vicuna_v1.1" 
    --report_to "azure_ml" 
    --run_name "Deita-7b" 
    --gradient_checkpointing True 
    --bf16 True
  