{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiscuser/.conda/envs/handbook_01/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"phi_7B_iter_65560_beta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLGv4Tokenizer(name_or_path='', vocab_size=100277, model_max_length=8192, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(path, trust_remote_code=True, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(model.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "100257\n",
      "None\n",
      "None\n",
      "<|endoftext|>\n",
      "100257\n",
      "[b'T', b'uring', b' Models', b' are', b' awesome', b'.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiscuser/.conda/envs/handbook_01/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> new block_sparse_attn op constructed with config: n_heads=32, max_seq_len=8192, sparse_block_size=64, local_blocks=16, vert_stride=8, homo_head=False, active_head_range=None, kwargs={'kernel_block_size': 64, 'inference': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiscuser/.cache/huggingface/modules/transformers_modules/phi_7B_iter_65560_beta/triton_flash_blocksparse_attn.py:71: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  x = [xi.to_sparse_csr() for xi in x]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate welcome in Hindi, and the word for welcome in Tamil is “vandha.” The word for welcome in Bengali is “shoncho.” The word for welcome in Kannada is “nannu.” The word for welcome in Malayalam is “vandha.” The word for welcome in Oriya is “shoncho.” The word for welcome in Punjabi is “shoncho.” The word for welcome in Sanskrit is “vandha.” The word for welcome in Telugu is “nannu.” The word for welcome in Tamil is “vandha.” The word for welcome in Urdu is “shoncho.” The word for welcome in Bengali is “shoncho.” The word for welcome in Kannada is “nannu.” The word for welcome in Malayalam is “vandha.” The word for welcome in Oriya is “shoncho.” The word for welcome in Punjabi is “shoncho.” The word for welcome in Sanskrit is “vandha.” The word for welcome in Telugu is “nannu.” The word for welcome in Tamil is “vandha.” The word for welcome in Urdu is “shoncho.” The word for welcome in Bengali is “shoncho\n"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.bos_token)\n",
    "print(tokenizer.bos_token_id)\n",
    "\n",
    "tokens = tokenizer.tokenize(\"Turing Models are awesome.\")\n",
    "print(tokens)\n",
    "\n",
    "# generate\n",
    "text = \"translate welcome in Hindi\"\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", return_attention_mask=True).to(DEVICE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.0001)\n",
    "\n",
    "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLGv4ForCausalLM(\n",
      "  (model): TLGv4Model(\n",
      "    (embed_tokens): Embedding(100352, 4096)\n",
      "    (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x TLGv4DecoderLayer(\n",
      "        (self_attn): TLGv4SelfAttention(\n",
      "          (query_key_value): Linear(in_features=4096, out_features=6144, bias=True)\n",
      "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): TLGv4MLP(\n",
      "          (up_proj): Linear(in_features=4096, out_features=28672, bias=True)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=100352, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handbook_02_phi3_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
